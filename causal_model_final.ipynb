{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# !{sys.executable} -m pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime, time\n",
    "import numpy as np\n",
    "import os, re, json, psutil\n",
    "import copy\n",
    "import warnings\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, f1_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelrobles/anaconda3/envs/mynewenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "from causallearn.search.ConstraintBased.FCI import fci\n",
    "from causallearn.utils.cit import kci, CIT\n",
    "from causallearn.utils.PCUtils.BackgroundKnowledge import BackgroundKnowledge\n",
    "from causallearn.graph.GeneralGraph import GeneralGraph\n",
    "from causallearn.graph.GraphNode import GraphNode\n",
    "from causallearn.utils.PDAG2DAG import pdag2dag\n",
    "\n",
    "from dowhy import gcm\n",
    "from dowhy.gcm import interventional_samples, AdditiveNoiseModel\n",
    "from dowhy.gcm.causal_mechanisms import StochasticModel\n",
    "from dowhy.gcm.ml import SklearnRegressionModel, SklearnClassificationModel\n",
    "from dowhy.gcm.auto import AssignmentQuality\n",
    "\n",
    "from notears.linear import notears_linear\n",
    "from notears.nonlinear import notears_nonlinear, NotearsMLP\n",
    "\n",
    "from lingam import LiM\n",
    "import pydot\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_pydot import to_pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 1000)\n",
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"     # prevent inner OpenMP threads\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"     # same for MKL\n",
    "os.environ[\"JOBLIB_MULTIPROCESSING\"] = \"0\"\n",
    "os.environ[\"JOBLIB_TEMP_FOLDER\"] = \"/tmp\"  # optional safety for temp files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>is_female</th>\n",
       "      <th>long_covid</th>\n",
       "      <th>me_cfs</th>\n",
       "      <th>fibromyalgia</th>\n",
       "      <th>dysautonomia</th>\n",
       "      <th>infection_episode</th>\n",
       "      <th>period_at_covid_start</th>\n",
       "      <th>acute_num_symp_prop</th>\n",
       "      <th>post_num_symp_prop</th>\n",
       "      <th>pre_num_symp_prop</th>\n",
       "      <th>acute_symp_sev_prop</th>\n",
       "      <th>post_symp_sev_prop</th>\n",
       "      <th>pre_symp_sev_prop</th>\n",
       "      <th>acute_symp_freq_prop</th>\n",
       "      <th>post_symp_freq_prop</th>\n",
       "      <th>pre_symp_freq_prop</th>\n",
       "      <th>pre_emotionally_stressful</th>\n",
       "      <th>pre_hr_variability</th>\n",
       "      <th>pre_mentally_demanding</th>\n",
       "      <th>pre_physically_active</th>\n",
       "      <th>pre_resting_hr</th>\n",
       "      <th>pre_sleep</th>\n",
       "      <th>acute_emotionally_stressful</th>\n",
       "      <th>acute_hr_variability</th>\n",
       "      <th>acute_mentally_demanding</th>\n",
       "      <th>acute_physically_active</th>\n",
       "      <th>acute_resting_hr</th>\n",
       "      <th>acute_sleep</th>\n",
       "      <th>post_emotionally_stressful</th>\n",
       "      <th>post_hr_variability</th>\n",
       "      <th>post_mentally_demanding</th>\n",
       "      <th>post_physically_active</th>\n",
       "      <th>post_resting_hr</th>\n",
       "      <th>post_sleep</th>\n",
       "      <th>pre_funcap_score</th>\n",
       "      <th>post_funcap_score</th>\n",
       "      <th>pre_crash</th>\n",
       "      <th>acute_crash</th>\n",
       "      <th>post_crash</th>\n",
       "      <th>acute_noncovid_infection</th>\n",
       "      <th>post_noncovid_infection</th>\n",
       "      <th>pre_noncovid_infection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.007278</td>\n",
       "      <td>0.037508</td>\n",
       "      <td>0.880572</td>\n",
       "      <td>0.488509</td>\n",
       "      <td>0.952535</td>\n",
       "      <td>0.344357</td>\n",
       "      <td>-0.109665</td>\n",
       "      <td>0.167581</td>\n",
       "      <td>-0.158928</td>\n",
       "      <td>-1.560641</td>\n",
       "      <td>0.660555</td>\n",
       "      <td>-1.534290</td>\n",
       "      <td>-2.018847</td>\n",
       "      <td>0.690702</td>\n",
       "      <td>0.982801</td>\n",
       "      <td>-1.156195</td>\n",
       "      <td>0.650428</td>\n",
       "      <td>-1.545455</td>\n",
       "      <td>-1.696970</td>\n",
       "      <td>0.962482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.303714</td>\n",
       "      <td>0.668402</td>\n",
       "      <td>-1.536009</td>\n",
       "      <td>-2.0125</td>\n",
       "      <td>1.078761</td>\n",
       "      <td>1.257576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.411765</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.892934</td>\n",
       "      <td>0.861503</td>\n",
       "      <td>0.880572</td>\n",
       "      <td>1.428591</td>\n",
       "      <td>0.971924</td>\n",
       "      <td>0.816596</td>\n",
       "      <td>1.057650</td>\n",
       "      <td>1.067037</td>\n",
       "      <td>1.224172</td>\n",
       "      <td>0.747051</td>\n",
       "      <td>-0.461927</td>\n",
       "      <td>0.730861</td>\n",
       "      <td>1.453629</td>\n",
       "      <td>0.490890</td>\n",
       "      <td>0.942207</td>\n",
       "      <td>1.030534</td>\n",
       "      <td>-0.448321</td>\n",
       "      <td>0.512397</td>\n",
       "      <td>0.501377</td>\n",
       "      <td>0.097147</td>\n",
       "      <td>2.276923</td>\n",
       "      <td>1.153667</td>\n",
       "      <td>-0.487816</td>\n",
       "      <td>0.777484</td>\n",
       "      <td>1.0500</td>\n",
       "      <td>0.507806</td>\n",
       "      <td>1.369464</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  is_female  long_covid  me_cfs  fibromyalgia  dysautonomia  \\\n",
       "0  0.529412          1           1       1             0             0   \n",
       "1  0.411765          0           1       0             0             0   \n",
       "\n",
       "   infection_episode  period_at_covid_start  acute_num_symp_prop  \\\n",
       "0                0.0                    0.0            -0.007278   \n",
       "1                0.0                    0.0             0.892934   \n",
       "\n",
       "   post_num_symp_prop  pre_num_symp_prop  acute_symp_sev_prop  \\\n",
       "0            0.037508           0.880572             0.488509   \n",
       "1            0.861503           0.880572             1.428591   \n",
       "\n",
       "   post_symp_sev_prop  pre_symp_sev_prop  acute_symp_freq_prop  \\\n",
       "0            0.952535           0.344357             -0.109665   \n",
       "1            0.971924           0.816596              1.057650   \n",
       "\n",
       "   post_symp_freq_prop  pre_symp_freq_prop  pre_emotionally_stressful  \\\n",
       "0             0.167581           -0.158928                  -1.560641   \n",
       "1             1.067037            1.224172                   0.747051   \n",
       "\n",
       "   pre_hr_variability  pre_mentally_demanding  pre_physically_active  \\\n",
       "0            0.660555               -1.534290              -2.018847   \n",
       "1           -0.461927                0.730861               1.453629   \n",
       "\n",
       "   pre_resting_hr  pre_sleep  acute_emotionally_stressful  \\\n",
       "0        0.690702   0.982801                    -1.156195   \n",
       "1        0.490890   0.942207                     1.030534   \n",
       "\n",
       "   acute_hr_variability  acute_mentally_demanding  acute_physically_active  \\\n",
       "0              0.650428                 -1.545455                -1.696970   \n",
       "1             -0.448321                  0.512397                 0.501377   \n",
       "\n",
       "   acute_resting_hr  acute_sleep  post_emotionally_stressful  \\\n",
       "0          0.962482     1.000000                   -1.303714   \n",
       "1          0.097147     2.276923                    1.153667   \n",
       "\n",
       "   post_hr_variability  post_mentally_demanding  post_physically_active  \\\n",
       "0             0.668402                -1.536009                 -2.0125   \n",
       "1            -0.487816                 0.777484                  1.0500   \n",
       "\n",
       "   post_resting_hr  post_sleep  pre_funcap_score  post_funcap_score  \\\n",
       "0         1.078761    1.257576               0.0                0.0   \n",
       "1         0.507806    1.369464               1.0                1.0   \n",
       "\n",
       "   pre_crash  acute_crash  post_crash  acute_noncovid_infection  \\\n",
       "0        1.0          1.0         1.0                         0   \n",
       "1        0.0          0.0         0.0                         0   \n",
       "\n",
       "   post_noncovid_infection  pre_noncovid_infection  \n",
       "0                        0                       0  \n",
       "1                        0                       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_type = 'loose'\n",
    "file_name_std = 'robust_std_data_for_model_'+file_type+'.csv'\n",
    "\n",
    "final_df_std = pd.read_csv(file_name_std)\n",
    "final_df_std.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optional helper distributions ---\n",
    "class GaussianDistribution(StochasticModel):\n",
    "    def __init__(self):\n",
    "        self.mean_ = None\n",
    "        self.std_ = None\n",
    "\n",
    "    def fit(self, X: np.ndarray):\n",
    "        X = np.array(X).flatten()\n",
    "        self.mean_ = np.mean(X)\n",
    "        self.std_ = np.std(X)\n",
    "        if self.std_ == 0:\n",
    "            self.std_ = 1e-6  # avoid degenerate distribution\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Return parameters in a uniform dict format.\"\"\"\n",
    "        return {\"mean\": self.mean_, \"std\": self.std_}\n",
    "\n",
    "    def draw_samples(self, n):\n",
    "        return np.random.normal(self.mean_, self.std_, size=(n, 1))\n",
    "\n",
    "    def evaluate(self, X: np.ndarray) -> np.ndarray:\n",
    "        # Identity for stochastic-only nodes (no parents)\n",
    "        return X\n",
    "\n",
    "    def clone(self):\n",
    "        new = GaussianDistribution()\n",
    "        new.mean_, new.std_ = self.mean_, self.std_\n",
    "        return new\n",
    "    \n",
    "class GaussianMixtureDistribution:\n",
    "    def __init__(self, n_components=2):\n",
    "        self.model = GaussianMixture(n_components=n_components)\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = np.asarray(X)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        self.model.fit(X)\n",
    "\n",
    "    def draw_samples(self, n):\n",
    "        return self.model.sample(n)[0]\n",
    "\n",
    "    def clone(self):\n",
    "        new = GaussianMixtureDistribution(n_components=self.model.n_components)\n",
    "        new.model.means_ = np.copy(self.model.means_)\n",
    "        new.model.covariances_ = np.copy(self.model.covariances_)\n",
    "        new.model.weights_ = np.copy(self.model.weights_)\n",
    "        return new\n",
    "\n",
    "    def parameters(self):\n",
    "        return {\n",
    "            \"weights\": self.model.weights_.tolist() if hasattr(self.model, \"weights_\") else None,\n",
    "            \"means\": self.model.means_.tolist() if hasattr(self.model, \"means_\") else None,\n",
    "            \"covariances\": self.model.covariances_.tolist() if hasattr(self.model, \"covariances_\") else None\n",
    "        }\n",
    "\n",
    "class BetaDistribution:\n",
    "    def __init__(self):\n",
    "        self.beta = beta\n",
    "        self.a = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = np.clip(X.flatten(), 1e-6, 1-1e-6)\n",
    "        X = np.asarray(X)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        self.a, self.b, _, _ = self.beta.fit(X, floc=0, fscale=1)\n",
    "\n",
    "    def draw_samples(self, n):\n",
    "        return self.beta.rvs(self.a, self.b, size=n).reshape(-1, 1)\n",
    "\n",
    "    def clone(self):\n",
    "        new = BetaDistribution()\n",
    "        new.a, new.b = self.a, self.b\n",
    "        return new\n",
    "\n",
    "    def parameters(self):\n",
    "        return {\"a\": self.a, \"b\": self.b}\n",
    "\n",
    "class BernoulliDistribution:\n",
    "    def __init__(self):\n",
    "        self.p = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = np.asarray(X).flatten()\n",
    "        self.p = np.mean(X)\n",
    "\n",
    "    def draw_samples(self, n):\n",
    "        return np.random.binomial(1, self.p, size=n).reshape(-1, 1)\n",
    "\n",
    "    def clone(self):\n",
    "        new = BernoulliDistribution()\n",
    "        new.p = self.p\n",
    "        return new\n",
    "\n",
    "    def parameters(self):\n",
    "        return {\"p\": self.p}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Background Knowledge Builder ===\n",
    "def build_background_knowledge_cl(var_names, added_required=None, added_forbidden=None):\n",
    "    graph_nodes = {v: GraphNode(v) for v in var_names}\n",
    "    bk = BackgroundKnowledge()\n",
    "\n",
    "    # --- Variable groups ---\n",
    "    post_vars = [c for c in var_names if c.startswith(\"post_\")]\n",
    "    acute_vars = [c for c in var_names if c.startswith(\"acute_\")]\n",
    "    pre_vars = [c for c in var_names if c.startswith(\"pre_\")]\n",
    "    pre_or_acute_vars = pre_vars + acute_vars\n",
    "    post_or_acute_vars = post_vars + acute_vars\n",
    "\n",
    "    # --- Root nodes ---\n",
    "    all_roots = ['age', 'is_female', 'me_cfs', 'long_covid', 'fibromyalgia', 'dysautonomia', 'infection_episode']\n",
    "    strict_roots = ['age', 'is_female', 'infection_episode']\n",
    "    relaxed_roots = [r for r in all_roots if r not in strict_roots]\n",
    "\n",
    "    # Strict roots: forbid all incoming edges\n",
    "    for v1 in var_names:\n",
    "        for v2 in strict_roots:\n",
    "            if v1 != v2:\n",
    "                bk.add_forbidden_by_node(graph_nodes[v1], graph_nodes[v2])\n",
    "\n",
    "    # Relaxed roots: forbid incoming edges from non-root nodes\n",
    "    for v1 in var_names:\n",
    "        for v2 in relaxed_roots:\n",
    "            if v1 != v2 and v1 not in all_roots:\n",
    "                bk.add_forbidden_by_node(graph_nodes[v1], graph_nodes[v2])\n",
    "\n",
    "    # Treatment root node: forbid outgoing edges to relaxed root nodes\n",
    "    for v2 in relaxed_roots:\n",
    "        bk.add_forbidden_by_node(graph_nodes['infection_episode'], graph_nodes[v2]) # Forbid future features from being influenced by infection_episode\n",
    "\n",
    "    # --- Required edges  ---\n",
    "    bk.add_required_by_node(graph_nodes['is_female'], graph_nodes['me_cfs'])\n",
    "    bk.add_required_by_node(graph_nodes['is_female'], graph_nodes['period_at_covid_start'])\n",
    "    bk.add_required_by_node(graph_nodes['age'], graph_nodes['period_at_covid_start'])\n",
    "\n",
    "    # Suggestive relationships from evaluation\n",
    "    if added_required:\n",
    "        for edge in added_required:\n",
    "            bk.add_required_by_node(graph_nodes[edge[0]], graph_nodes[edge[1]])\n",
    "\n",
    "    # --- Temporal forbidden edges ---\n",
    "    for v1 in post_vars:\n",
    "        for v2 in pre_or_acute_vars:\n",
    "            bk.add_forbidden_by_node(graph_nodes[v1], graph_nodes[v2])\n",
    "    for v1 in acute_vars:\n",
    "        for v2 in pre_vars:\n",
    "            bk.add_forbidden_by_node(graph_nodes[v1], graph_nodes[v2])\n",
    "    for v1 in post_or_acute_vars:\n",
    "        bk.add_forbidden_by_node(graph_nodes[v1], graph_nodes['period_at_covid_start']) # Forbid future features from influencing period_at_covid_start\n",
    "    for v2 in pre_vars:\n",
    "        bk.add_forbidden_by_node(graph_nodes['period_at_covid_start'], graph_nodes[v2]) # Forbid previous features from being influenced by period_at_covid_start\n",
    "\n",
    "    # Suggestive relationships from evaluation\n",
    "    if added_forbidden:\n",
    "        for edge in added_forbidden:\n",
    "            bk.add_forbidden_by_node(graph_nodes[edge[0]], graph_nodes[edge[1]])\n",
    "\n",
    "    print(f\"# Forbidden edges: {len(bk.forbidden_rules_specs)}\")\n",
    "    print(f\"# Required edges: {len(bk.required_rules_specs)}\")\n",
    "    \n",
    "    return bk\n",
    "\n",
    "def build_background_knowledge(columns, added_required=None, added_forbidden=None):\n",
    "    \"\"\"\n",
    "    Build forbidden and required edge sets based on temporal\n",
    "    and domain constraints.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    columns : list\n",
    "        Variable names\n",
    "    \"\"\"\n",
    "    forbidden = []\n",
    "    required = []\n",
    "\n",
    "    # --- Variable groups ---\n",
    "    post_vars = [c for c in columns if c.startswith(\"post_\")]\n",
    "    acute_vars = [c for c in columns if c.startswith(\"acute_\")]\n",
    "    pre_vars = [c for c in columns if c.startswith(\"pre_\")]\n",
    "    pre_or_acute_vars = pre_vars + acute_vars\n",
    "    post_or_acute_vars = post_vars + acute_vars\n",
    "\n",
    "    # --- Root nodes ---\n",
    "    all_roots = ['age', 'is_female', 'me_cfs', 'long_covid', 'fibromyalgia', 'dysautonomia', 'infection_episode']\n",
    "    strict_roots = ['age', 'is_female', 'infection_episode']\n",
    "    relaxed_roots = [r for r in all_roots if r not in strict_roots]\n",
    "\n",
    "    # Strict roots: forbid all incoming edges\n",
    "    for src in columns:\n",
    "        for tgt in strict_roots:\n",
    "            if src != tgt:\n",
    "                forbidden.append((src, tgt))\n",
    "\n",
    "    # Relaxed roots: forbid incoming edges from non-root nodes\n",
    "    for src in columns:\n",
    "        for tgt in relaxed_roots:\n",
    "            if src != tgt and src not in all_roots:\n",
    "                forbidden.append((src, tgt))\n",
    "\n",
    "    # Treatment root node: forbid outgoing edges to relaxed root nodes\n",
    "    for tgt in relaxed_roots:\n",
    "        forbidden.append((\"infection_episode\", tgt))\n",
    "\n",
    "    # --- Required edges ---\n",
    "    required += [\n",
    "        (\"is_female\", \"me_cfs\"),\n",
    "        (\"is_female\", \"period_at_covid_start\"),\n",
    "        (\"age\", \"period_at_covid_start\"),\n",
    "    ]\n",
    "\n",
    "    # Suggestive relationships from evaluation\n",
    "    if added_required:\n",
    "        for edge in added_required:\n",
    "            required += [(edge[0],edge[1])]\n",
    "\n",
    "    # --- Temporal forbidden edges ---\n",
    "    for src in post_vars:\n",
    "        for tgt in pre_or_acute_vars:\n",
    "            forbidden.append((src, tgt))\n",
    "\n",
    "    for src in acute_vars:\n",
    "        for tgt in pre_vars:\n",
    "            forbidden.append((src, tgt))\n",
    "\n",
    "    for src in post_or_acute_vars:\n",
    "        forbidden.append((src, \"period_at_covid_start\"))\n",
    "\n",
    "    for tgt in pre_vars:\n",
    "        forbidden.append((\"period_at_covid_start\", tgt))\n",
    "\n",
    "    # Suggestive relationships from evaluation\n",
    "    if added_forbidden:\n",
    "        for edge in added_forbidden:\n",
    "            forbidden.append(edge[0],edge[1])\n",
    "\n",
    "    return forbidden, required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Model Runner ===\n",
    "def run_model(model, df, outdir=\"graph_plots\", alpha=0.075, added_required=None, added_forbidden=None):\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # Initialize variables\n",
    "    var_names = df.columns.tolist()\n",
    "    fitted_model = None\n",
    "    latent_confounders = set()\n",
    "\n",
    "    # --- PC ---\n",
    "    if model==\"pc\":\n",
    "        # Build background knowledge\n",
    "        bk = build_background_knowledge_cl(var_names, added_required=added_required, added_forbidden=added_forbidden)\n",
    "\n",
    "        # Run model\n",
    "        data_np = np.asarray(df, dtype=np.float64)\n",
    "        result = pc(\n",
    "            data_np,\n",
    "            indep_test=\"kci\",\n",
    "            alpha=alpha,\n",
    "            node_names=var_names,\n",
    "            background_knowledge=bk\n",
    "        )\n",
    "\n",
    "        # Handle tuple vs. direct graph\n",
    "        graph = result[0] if isinstance(result, tuple) else result\n",
    "        graph = graph.G if hasattr(graph, \"G\") else graph  # unwrap to GeneralGraph\n",
    "\n",
    "        # Convert CPDAG to DAG\n",
    "        dag_graph = pdag2dag(graph)\n",
    "\n",
    "        edges = {(e.get_node1().get_name(),\n",
    "                e.get_node2().get_name(),\n",
    "                (str(e.get_endpoint1()), str(e.get_endpoint2())))\n",
    "                for e in dag_graph.get_graph_edges()}\n",
    "\n",
    "    # --- FCI ---\n",
    "    elif model==\"fci\":\n",
    "        # Build background knowledge\n",
    "        bk = build_background_knowledge_cl(var_names, added_required=added_required, added_forbidden=added_forbidden)\n",
    "\n",
    "        # Run model\n",
    "        data_np = np.asarray(df, dtype=np.float64)\n",
    "        result = fci(\n",
    "            data_np,\n",
    "            depth=2,\n",
    "            independence_test_method=\"kci\",  # fisherz is fast and gives interpretable partial correlations, but can only be used on continuous vars\n",
    "            # kci models nonlinear or complex interactions\n",
    "            alpha=alpha,\n",
    "            max_path_length=3,\n",
    "            node_names=var_names,\n",
    "            background_knowledge=bk\n",
    "        )\n",
    "        \n",
    "        # Handle tuple vs. direct graph\n",
    "        graph = result[0] if isinstance(result, tuple) else result\n",
    "        graph = graph.G if hasattr(graph, \"G\") else graph  # unwrap to GeneralGraph\n",
    "\n",
    "        # Extract edges\n",
    "        edges = set()\n",
    "        for e in graph.get_graph_edges():\n",
    "            endpoint1 = str(e.get_endpoint1())\n",
    "            endpoint2 = str(e.get_endpoint2())\n",
    "            \n",
    "            # Only include if it's a directed edge (TAIL -> ARROW)\n",
    "            if endpoint1 == \"TAIL\" and endpoint2 == \"ARROW\":\n",
    "                edges.add((e.get_node1().get_name(), \n",
    "                                e.get_node2().get_name(), (endpoint1, endpoint2)))\n",
    "            elif endpoint1 == \"ARROW\" and endpoint2 == \"TAIL\":\n",
    "                edges.add((e.get_node2().get_name(), \n",
    "                                e.get_node1().get_name(), (endpoint2, endpoint1)))\n",
    "            # Save latent confounders\n",
    "            elif endpoint1 == \"ARROW\" and endpoint2 == \"ARROW\":\n",
    "                latent_confounders.add((e.get_node1().get_name(), \n",
    "                                e.get_node2().get_name(), (endpoint1, endpoint2)))\n",
    "\n",
    "    # --- LiM ---\n",
    "    elif model==\"lingam\":\n",
    "        # Identify discrete vs continuous variables\n",
    "        # Continuous (float) = 1, Discrete/binary/int = 0\n",
    "        dis_con = np.ones((1, len(var_names)))\n",
    "        for i, col in enumerate(var_names):\n",
    "            if df[col].nunique() <= 4:\n",
    "                dis_con[0, i] = 0  # discrete/binary variable\n",
    "            else:\n",
    "                dis_con[0, i] = 1  # continuous variable\n",
    "\n",
    "        # Build background knowledge\n",
    "        forbidden, required = build_background_knowledge(var_names, added_required=added_required, added_forbidden=added_forbidden)\n",
    "        forbidden_edges = set(forbidden)  # convert to set for fast lookup\n",
    "\n",
    "        # Run model\n",
    "        data_np = np.asarray(df, dtype=np.float64)\n",
    "        lingam_model = LiM( # best for mixed data types\n",
    "            lambda1=0.085,     # weaker L1 (default is 0.1) -> more edges allowed\n",
    "            max_iter=150,      # keep as is\n",
    "            h_tol=1e-8,        # keep acyclicity tolerance\n",
    "            rho_max=1e16,      # keep default rho\n",
    "            w_threshold=1e-6   # preserve as many small weights as possible (not 0 for speed)\n",
    "        )\n",
    "        lingam_model.fit(data_np, dis_con, only_global=True) # only_global = False allows local search and usually adds more edges \n",
    "        adj_matrix = lingam_model.adjacency_matrix_\n",
    "        fitted_model = lingam_model\n",
    "\n",
    "        # Zero out forbidden edges in adjacency (prune)\n",
    "        name_to_idx = {n: i for i, n in enumerate(var_names)}\n",
    "        for src, dst in list(forbidden_edges):\n",
    "            i = name_to_idx[src]; j = name_to_idx[dst]\n",
    "            adj_matrix[i, j] = 0.0\n",
    "\n",
    "        # Extract edges\n",
    "        edges = set()\n",
    "        for i, src in enumerate(var_names):\n",
    "            for j, dst in enumerate(var_names):\n",
    "                if adj_matrix[i, j] != 0:\n",
    "                    edges.add((src, dst, (\"TAIL\", \"ARROW\")))\n",
    "\n",
    "        # Save fitted model and pruned adjacency so you can use for diagnostics/SEM\n",
    "        fitted_model = {\"lingam\": lingam_model, \"adj_matrix_pruned\": adj_matrix}\n",
    "    \n",
    "    # --- NOTEARS ---\n",
    "    elif model in [\"notears_linear\", \"notears_nonlinear\"]:\n",
    "\n",
    "        # Build background knowledge\n",
    "        forbidden, required = build_background_knowledge(var_names, added_required=added_required, added_forbidden=added_forbidden)\n",
    "        forbidden_edges = set(forbidden)  # convert to set for fast lookup\n",
    "\n",
    "        # Run model\n",
    "        df_for_notears = np.asarray(df, dtype=np.float64) \n",
    "        if model == \"notears_linear\":\n",
    "            # Run NOTEARS linear\n",
    "            W_est = notears_linear(\n",
    "                df_for_notears,\n",
    "                lambda1=0.02,       # the higher the value, the more edges are pruned\n",
    "                loss_type='l2',\n",
    "                max_iter=100,       # keep as is for accuracy\n",
    "                h_tol=1e-8,         # don't loosen -- will reduce accuracy\n",
    "                rho_max=1e16,       # default 1e16 â†’ decreasing will stop earlier if diverging\n",
    "                w_threshold=1e-6    # preserve as many small weights as possible (not 0 for speed)\n",
    "            )\n",
    "        else:\n",
    "            # Run NOTEARS nonlinear\n",
    "            d = df_for_notears.shape[1] # number of features\n",
    "            torch.set_default_dtype(torch.double)\n",
    "            model_1 = NotearsMLP(dims=[d, 15, 1], bias=True) # 8 hidden units, 1 output per variable\n",
    "            W_est = notears_nonlinear(\n",
    "                model_1,\n",
    "                df_for_notears.astype(np.double),\n",
    "                lambda1=0.002,      # the higher the value, the more edges are pruned\n",
    "                lambda2=0.07,       # add a bit of L2 regularization to stabilize edges across runs -- very sensitive, will affect run time\n",
    "                max_iter=100,       # default 100 â†’ fewer dual steps for efficiency\n",
    "                h_tol=1e-8,         # don't loosen -- will reduce accuracy\n",
    "                rho_max=1e16,       # keep as is\n",
    "                w_threshold=1e-6    # preserve as many small weights as possible (not 0 for speed)\n",
    "            )\n",
    "            # Checking thresholding\n",
    "            low_edges = [(var_names[i], var_names[j], W_est[i,j])\n",
    "                for i in range(len(var_names)) for j in range(len(var_names))\n",
    "                if abs(W_est[i,j]) < 0.05 and W_est[i,j] != 0]\n",
    "            print(f\"Number of low-magnitude edges (<0.05): {len(low_edges)}\")\n",
    "        fitted_model = W_est\n",
    "\n",
    "        # Adding edges\n",
    "        edges_before = np.sum(W_est != 0)\n",
    "        edges_pruned = 0\n",
    "        edges = set()\n",
    "        for i, src in enumerate(var_names):\n",
    "            for j, dst in enumerate(var_names):\n",
    "                if W_est[i, j] != 0 and (src, dst) not in forbidden_edges:\n",
    "                    edges.add((src, dst, (\"TAIL\", \"ARROW\")))\n",
    "                elif W_est[i, j] != 0 and (src, dst) in forbidden_edges:\n",
    "                    edges_pruned += 1\n",
    "        print(f\"Edges before pruning: {edges_before}, pruned forbidden: {edges_pruned}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model}\")\n",
    "    \n",
    "    return {\"edges\": edges, \"latent_confounders\": latent_confounders}\n",
    "\n",
    "def run_bootstrap_discovery(model, data, alpha, bootstrap, stage_level, required=None, forbidden=None):\n",
    "    \"\"\"Run causal discovery bootstraps and return edge stability + weights.\"\"\"\n",
    "\n",
    "    edge_results = []\n",
    "    edge_presence, conf_presence = defaultdict(list), defaultdict(list)\n",
    "\n",
    "    def single_run(seed):\n",
    "        print(\"Starting single causal discovery run\")\n",
    "        print(\"Worker memory (MB):\", psutil.Process(os.getpid()).memory_info().rss / 1e6)\n",
    "\n",
    "        # (1) Bootstrap â€” sampling with replacement (same number of rows)\n",
    "        df_boot = data.sample(frac=1, replace=True, random_state=int(seed)).copy() # sample with replacement and produce same number of rows\n",
    "        df_boot = df_boot.reset_index(drop=True)\n",
    "\n",
    "        # (2) Subsample â€” cap sample size to speed up KCI\n",
    "        if model in [\"pc\", \"fci\"]:\n",
    "            max_samples = 200\n",
    "            if len(df_boot) > max_samples:\n",
    "                df_boot = df_boot.sample(n=max_samples, random_state=int(seed))\n",
    "\n",
    "        # Run the discovery model\n",
    "        return run_model(model, df_boot, alpha=alpha, added_required=required, added_forbidden=forbidden)\n",
    "\n",
    "    # Create unique seeds for each job to avoid RNG collisions\n",
    "    seeds = [np.random.SeedSequence(42).spawn(bootstrap)[i].generate_state(1)[0] for i in range(bootstrap)]\n",
    "\n",
    "    # Run in parallel\n",
    "    run_outputs = Parallel(n_jobs=6, backend=\"loky\", max_tasks_per_child=1)( # define max so that bootstraps don't overrun memory\n",
    "        delayed(single_run)(int(seed)) for seed in seeds\n",
    "    )\n",
    "    print(\"Bootstraps finished. Now aggregating...\")\n",
    "    print(len(run_outputs))\n",
    "\n",
    "    # Aggregate results\n",
    "    for run_out in run_outputs:\n",
    "        for (u, v, _) in run_out[\"edges\"]:\n",
    "            edge_presence[(u, v)].append(1.0)\n",
    "        if isinstance(run_out.get(\"latent_confounders\"), set):\n",
    "            for (u, v, _) in run_out[\"latent_confounders\"]:\n",
    "                conf_presence[(u, v)].append(1.0)\n",
    "\n",
    "    stability = {e: np.sum(pres) / bootstrap for e, pres in edge_presence.items()}\n",
    "    confounders = {e: np.sum(pres) / bootstrap for e, pres in conf_presence.items()}\n",
    "\n",
    "    final_edges = [e for e, p in stability.items()]\n",
    "\n",
    "    edge_results.append({\n",
    "        \"model\": model,\n",
    "        \"stage\": stage_level,\n",
    "        \"alpha\": alpha,\n",
    "        \"n_bootstrap\": bootstrap,\n",
    "        \"num_edges\": len(final_edges),\n",
    "        \"edges\": final_edges,\n",
    "        \"stability\": stability,\n",
    "        \"confounders\": confounders\n",
    "    })\n",
    "    return edge_results, stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Edge Stability ===\n",
    "def summarize_edge_stability(results_df):\n",
    "    \"\"\"\n",
    "    Summarize edge stability with table per (model, feature_set, alpha, n_bootstrap).\n",
    "    \"\"\"\n",
    "    summaries, conf_summaries = [], []\n",
    "\n",
    "    for i, row in results_df.iterrows():\n",
    "        stability = row[\"stability\"]\n",
    "        confounders = row[\"confounders\"]\n",
    "        model = row[\"model\"]\n",
    "        edges = list(stability.keys())\n",
    "        conf_edges = list(confounders.keys())\n",
    "\n",
    "        for edge in edges:\n",
    "            record = {\n",
    "                \"model\": row[\"model\"],\n",
    "                \"n_bootstrap\": row[\"n_bootstrap\"],\n",
    "                \"edge\": edge,\n",
    "                \"freq\": stability[edge]\n",
    "            }\n",
    "            summaries.append(record)\n",
    "        \n",
    "        for c_edge in conf_edges:\n",
    "            conf_record = {\n",
    "                \"model\": row[\"model\"],\n",
    "                \"n_bootstrap\": row[\"n_bootstrap\"],\n",
    "                \"edge\": c_edge,\n",
    "                \"freq\": confounders[c_edge]\n",
    "            }\n",
    "            conf_summaries.append(conf_record)\n",
    "\n",
    "    return pd.DataFrame(summaries), pd.DataFrame(conf_summaries)\n",
    "\n",
    "# === Consensus Graphs ===\n",
    "def build_consensus_graph(model, stability, required_edges, freq_threshold=0.5, added_required=None):\n",
    "    G = nx.DiGraph()\n",
    "    forced_edges = []\n",
    "        \n",
    "    for (u, v), stats in stability.items():\n",
    "        freq = stats\n",
    "        if freq >= freq_threshold:\n",
    "            G.add_edge(u, v, weight=freq)\n",
    "\n",
    "    # Add required edges if missing, and note them\n",
    "    required_set = set(required_edges)\n",
    "    added_required_set = set(added_required) if added_required else set()\n",
    "    for src, tgt in required_edges:\n",
    "        if not G.has_edge(src, tgt):\n",
    "            G.add_edge(src, tgt, weight=1, forced=True)\n",
    "            forced_edges.append((src, tgt))\n",
    "        else:\n",
    "            # Even if discovered, mark it\n",
    "            G[src][tgt][\"forced\"] = True  \n",
    "\n",
    "    if forced_edges:\n",
    "        print(f\"âš ï¸ Forced edges added: {forced_edges}\")\n",
    "\n",
    "    # Fixing cycles in graph\n",
    "    if not nx.is_directed_acyclic_graph(G):\n",
    "        print(\"âš ï¸ Cycle detected in consensus graph!\")\n",
    "\n",
    "    # More efficient cycle breaking: break one cycle at a time using prioritization\n",
    "    # Safety cap: don't remove more than 2 * E edges (shouldn't be needed)\n",
    "    max_removals = max(1000, 2 * G.number_of_edges())\n",
    "    removals = 0\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Get one cycle (NetworkX raises if none)\n",
    "            cycle_edges = nx.find_cycle(G, orientation=\"original\")\n",
    "            # nx.find_cycle returns list of (u, v, dir) tuples\n",
    "            # convert to simple cycle node order for clarity\n",
    "            cycle_nodes = []\n",
    "            for u, v, _ in cycle_edges:\n",
    "                cycle_nodes.append(u)\n",
    "            # append last node if not present\n",
    "            if cycle_nodes and cycle_nodes[-1] != cycle_edges[-1][1]:\n",
    "                cycle_nodes.append(cycle_edges[-1][1])\n",
    "\n",
    "            # gather edges (u->v) around the cycle in order\n",
    "            edge_triplets = []\n",
    "            for u, v in zip(cycle_nodes, cycle_nodes[1:] + [cycle_nodes[0]]):\n",
    "                if G.has_edge(u, v):\n",
    "                    attr = G[u][v]\n",
    "                    weight = attr.get(\"weight\", 0.0)\n",
    "                    is_required = (u, v) in required_set\n",
    "                    is_added_required = (u, v) in added_required_set\n",
    "                    edge_triplets.append({\n",
    "                        \"edge\": (u, v),\n",
    "                        \"weight\": weight,\n",
    "                        \"is_required\": is_required,\n",
    "                        \"is_added_required\": is_added_required,\n",
    "                        \"forced\": attr.get(\"forced\", False)\n",
    "                    })\n",
    "            \n",
    "            print(f\"Total edges BEFORE removal: {G.number_of_edges()}\")\n",
    "\n",
    "            # Skip empty cycles that have been resolved\n",
    "            if not edge_triplets:\n",
    "                print(f\"âš ï¸ No actionable edges found for cycle {cycle_nodes}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            # 1. Prefer to remove non-required edges first\n",
    "            non_required = [e for e in edge_triplets if not e[\"is_required\"]]\n",
    "            if non_required:\n",
    "                edge_to_remove = min(non_required, key=lambda x: x[\"weight\"])\n",
    "            else:\n",
    "                # 2. If all are required, prefer to remove those in added_required\n",
    "                in_added_required = [e for e in edge_triplets if e[\"is_added_required\"]]\n",
    "                if in_added_required:\n",
    "                    edge_to_remove = min(in_added_required, key=lambda x: x[\"weight\"]) # TO DO: choose based on smallest feature importance?\n",
    "                else:\n",
    "                    # 3. If all are biologically required, just pick the lowest weight\n",
    "                    edge_to_remove = min(edge_triplets, key=lambda x: x[\"weight\"])\n",
    "\n",
    "            if edge_to_remove[\"is_required\"] and not edge_to_remove[\"is_added_required\"]:\n",
    "                print(\"ðŸš¨ WARNING: Removing a biologically required edge to break cycle!\")\n",
    "            u, v = edge_to_remove[\"edge\"]\n",
    "            w = G[u][v].get(\"weight\", None)\n",
    "            print(f\"Removing edge {(u, v, w)} to break cycle {cycle_nodes}\")\n",
    "            G.remove_edge(u, v)\n",
    "            removals += 1\n",
    "\n",
    "            print(f\"Final DAG edges: {G.number_of_edges()}\")\n",
    "            print(f\"   (removed {removals} edges to break cycles)\")\n",
    "\n",
    "            if removals >= max_removals:\n",
    "                raise RuntimeError(f\"Exceeded max removals ({max_removals}) while breaking cycles â€” aborting to avoid infinite loop.\")\n",
    "\n",
    "            # continue until no cycle\n",
    "            if nx.is_directed_acyclic_graph(G):\n",
    "                print(\"No more cycles detected in graph\")\n",
    "                break\n",
    "\n",
    "        except nx.exception.NetworkXNoCycle:\n",
    "            # no cycles left\n",
    "            break\n",
    "\n",
    "    # final check\n",
    "    if not nx.is_directed_acyclic_graph(G):\n",
    "        raise RuntimeError(\"Failed to produce DAG after cycle-breaking loop.\")\n",
    "            \n",
    "    # Apply this to all nodes in the NetworkX graph before converting to pydot\n",
    "    mapping = {node: sanitize_node_name(node) for node in G.nodes()} \n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "    return G\n",
    "\n",
    "def sanitize_node_name(node: str) -> str:\n",
    "    # Remove semicolons, parentheses, spaces, and replace illegal characters with underscore\n",
    "    node = node.replace(\";\", \"\").strip()\n",
    "    node = re.sub(r'[^A-Za-z0-9_]', '_', node)\n",
    "    return node\n",
    "\n",
    "def plot_consensus_graph(G, model, threshold=0.5, outdir=\"graph_plots\", stage_level=\"initial\", alpha=0.01, n_bootstrap=5, dpi=250):\n",
    "    \"\"\"\n",
    "    Plot a consensus graph using pydot (Graphviz) for cleaner layouts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to pydot\n",
    "    pydot_graph = to_pydot(G)\n",
    "\n",
    "    # Optional: set node styles/colors by phase\n",
    "    for node in pydot_graph.get_nodes():\n",
    "        original_name = node.get_name().strip('\"')\n",
    "        node.set_label(original_name)  # keeps readable name\n",
    "        if \"pre\" in original_name:\n",
    "            node.set_fillcolor(\"skyblue\")\n",
    "        elif \"acute\" in original_name:\n",
    "            node.set_fillcolor(\"lightgreen\")\n",
    "        elif \"post\" in original_name:\n",
    "            node.set_fillcolor(\"salmon\")\n",
    "        else:\n",
    "            node.set_fillcolor(\"gray\")\n",
    "        node.set_style(\"filled\")\n",
    "        node.set_fontsize(10)\n",
    "\n",
    "    # Style edges\n",
    "    for edge in pydot_graph.get_edges():\n",
    "        src = sanitize_node_name(edge.get_source().strip('\"'))\n",
    "        tgt = sanitize_node_name(edge.get_destination().strip('\"'))\n",
    "        forced = G[src][tgt].get(\"forced\", False)\n",
    "        weight = G[src][tgt].get(\"weight\", 0.5)\n",
    "\n",
    "        if forced:\n",
    "            # Bold red for prior knowledge edges\n",
    "            edge.set_color(\"red\")\n",
    "            edge.set_penwidth(2)\n",
    "            edge.set_style(\"bold\")\n",
    "        else:\n",
    "            # Scale penwidth by weight (bootstrap stability)\n",
    "            edge.set_color(\"black\")\n",
    "            edge.set_penwidth(1 + 2*weight)  # thicker if higher stability\n",
    "            edge.set_style(\"solid\")\n",
    "\n",
    "    # Add title as a graph label\n",
    "    if model==\"fci\" or model==\"pc\":\n",
    "        title=f\"{model.upper()} | alpha={alpha} | bootstraps={n_bootstrap} | thr={threshold}\"\n",
    "    else:  \n",
    "        title=f\"{model.upper()} | bootstraps={n_bootstrap} | thr={threshold}\"\n",
    "    pydot_graph.set_label(title)\n",
    "    pydot_graph.set_labelloc(\"t\")\n",
    "    pydot_graph.set_labeljust(\"c\")\n",
    "\n",
    "    # File naming convention\n",
    "    if stage_level==\"initial\":\n",
    "        if model in [\"pc\", \"fci\"]:\n",
    "            fname_base = f\"{model}_a{alpha}_b{n_bootstrap}_thr{threshold}\"\n",
    "        else:\n",
    "            fname_base = f\"{model}_b{n_bootstrap}_thr{threshold}\"\n",
    "    else:\n",
    "        if model in [\"pc\", \"fci\"]:\n",
    "            fname_base = f\"{model}_a{alpha}_b{n_bootstrap}_thr{threshold}_2\"\n",
    "        else:\n",
    "            fname_base = f\"{model}_b{n_bootstrap}_thr{threshold}_2\"\n",
    "\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # Export\n",
    "    pdf_path = os.path.join(outdir, fname_base + \".pdf\")\n",
    "    pydot_graph.write_pdf(pdf_path)\n",
    "\n",
    "    print(f\"Saved Graphviz consensus graph to {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Node Evaluation ===\n",
    "def evaluate_nodes(scm, data, nodes, expand, causal_mech=True, mech_base=True, overall_kl=False, invert_assumpt=False, causal_struct=False):\n",
    "    \"\"\"Safely evaluate for any cut of nodes.\"\"\"\n",
    "\n",
    "    if isinstance(nodes, str):\n",
    "        nodes = [nodes]\n",
    "\n",
    "    # Always include ancestors (required for non-root mechanisms)\n",
    "    if expand:\n",
    "        ancestors = set()\n",
    "        for node in nodes:\n",
    "            ancestors |= nx.ancestors(scm.graph, node)\n",
    "        expanded_nodes = list(set(nodes) | ancestors)\n",
    "    else:\n",
    "        expanded_nodes = nodes\n",
    "\n",
    "    # Make a subgraph with only those nodes\n",
    "    print(\"Setting causal mechanisms\")\n",
    "    scm_sub = gcm.InvertibleStructuralCausalModel(scm.graph.subgraph(expanded_nodes).copy())\n",
    "    for node in expanded_nodes:\n",
    "        scm_sub.set_causal_mechanism(node, scm.causal_mechanism(node))\n",
    "\n",
    "    # Run evaluation\n",
    "    result = gcm.evaluate_causal_model(\n",
    "        scm_sub,\n",
    "        data[expanded_nodes],\n",
    "        evaluate_causal_mechanisms=causal_mech,  # calculates normalized continuous ranked probability score (all nodes), MSE, normalized MSE, and R2 (continuous), and F1 (categorical)\n",
    "        compare_mechanism_baselines=mech_base, # compares the causal mechanisms with baseline models to see if there are model choices that perform significantly better\n",
    "        evaluate_invertibility_assumptions=invert_assumpt, # tests statistical independence between the reconstructed noise and the used input samples\n",
    "        evaluate_overall_kl_divergence=overall_kl, # tests KL divergence between the generated and the observed data\n",
    "        evaluate_causal_structure=causal_struct # evaluates to find substantial evidence to refute the causal graph based on the provided data\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "def describe_mechanism(mech, verbose=False):\n",
    "    \"\"\"Return human-readable description of a causal mechanism and its underlying model or distribution.\"\"\"\n",
    "    if mech is None:\n",
    "        return (\"None\", \"None\", {})\n",
    "\n",
    "    mech_type = type(mech).__name__\n",
    "    model_name, model_params = \"None\", {}\n",
    "\n",
    "    # --- Case 1: Regression-based mechanisms ---\n",
    "    if hasattr(mech, \"prediction_model\") and mech.prediction_model is not None:\n",
    "        pm = mech.prediction_model\n",
    "        inner_model = None\n",
    "        for attr in [\"model\", \"_model\", \"_sklearn_mdl\", \"sklearn_model\", \"estimator\", \"clf\", \"regressor\"]:\n",
    "            if hasattr(pm, attr) and getattr(pm, attr) is not None:\n",
    "                inner_model = getattr(pm, attr)\n",
    "                break\n",
    "        if inner_model is not None:\n",
    "            model_name = type(inner_model).__name__\n",
    "            if hasattr(inner_model, \"get_params\"):\n",
    "                model_params = inner_model.get_params()\n",
    "            else:\n",
    "                model_params = {k: v for k, v in vars(inner_model).items() if not k.startswith(\"_\")}\n",
    "        else:\n",
    "            model_name = type(pm).__name__\n",
    "\n",
    "    # --- Case 2: Classification-based mechanisms ---\n",
    "    elif hasattr(mech, \"classifier_model\") and mech.classifier_model is not None:\n",
    "        cm = mech.classifier_model\n",
    "        inner_model = None\n",
    "        for attr in [\"model\", \"_model\", \"_sklearn_mdl\", \"sklearn_model\", \"estimator\", \"clf\"]:\n",
    "            if hasattr(cm, attr) and getattr(cm, attr) is not None:\n",
    "                inner_model = getattr(cm, attr)\n",
    "                break\n",
    "        if inner_model is not None:\n",
    "            model_name = type(inner_model).__name__\n",
    "            if hasattr(inner_model, \"get_params\"):\n",
    "                model_params = inner_model.get_params()\n",
    "            else:\n",
    "                model_params = {k: v for k, v in vars(inner_model).items() if not k.startswith(\"_\")}\n",
    "        else:\n",
    "            model_name = type(cm).__name__\n",
    "\n",
    "    # --- Case 3: Custom distribution with .parameters() ---\n",
    "    elif hasattr(mech, \"parameters\"):\n",
    "        param_attr = mech.parameters\n",
    "        if callable(param_attr):\n",
    "            try:\n",
    "                model_params = param_attr()\n",
    "            except Exception:\n",
    "                model_params = {}\n",
    "        else:\n",
    "            model_params = param_attr\n",
    "        model_name = type(mech).__name__\n",
    "\n",
    "    # --- Case 4: Wrapped distribution inside ScipyDistribution / custom ---\n",
    "    if hasattr(mech, \"_parameters\") and \"dist\" in mech._parameters:\n",
    "        dist = mech._parameters[\"dist\"]\n",
    "        model_name = type(dist).__name__  # e.g., GaussianDistribution\n",
    "        # Extract parameters if they exist\n",
    "        if hasattr(dist, \"__dict__\"):\n",
    "            model_params = {k: v for k, v in vars(dist).items() if not k.startswith(\"_\")}\n",
    "\n",
    "    # --- Truncate large parameter sets for readability ---\n",
    "    if model_params and len(model_params) > 20:\n",
    "        model_params = {k: model_params[k] for k in list(model_params)[:15]}\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[describe_mechanism] {mech_type} -> {model_name}, params keys={list(model_params.keys())}\")\n",
    "\n",
    "    return mech_type, model_name, model_params\n",
    "\n",
    "def summarize_and_flag_nodes(model, eval_result, scm, kl_thr, crps_thr, f1_thr, nodes=None, causal_mech=True, invert_assumpt=False):\n",
    "    \"\"\"\n",
    "    Summarizes SCM evaluation into a node-level table, including:\n",
    "      - Mechanism type\n",
    "      - Number of parents\n",
    "      - MSE/RÂ²/CRPS where available\n",
    "      - Invertibility assumption rejected (TRUE/FALSE)\n",
    "      - Flag for nodes needing attention (invertibility rejected)\n",
    "    \"\"\"\n",
    "\n",
    "    rows = []\n",
    "    if isinstance(nodes, str):\n",
    "        nodes = [nodes]\n",
    "        \n",
    "    if nodes is None:\n",
    "        nodes = scm.graph.nodes\n",
    "\n",
    "    for node in nodes:\n",
    "        # Mechanism type\n",
    "        mech = scm.causal_mechanism(node)\n",
    "        mech_type, pred_model, model_params = describe_mechanism(mech)\n",
    "\n",
    "        # Parents count\n",
    "        n_parents = len(list(scm.graph.predecessors(node)))\n",
    "\n",
    "        passed_metric_threshold = None\n",
    "        # Performance metrics\n",
    "        if causal_mech:\n",
    "            perf = getattr(eval_result, \"mechanism_performances\", {}).get(node, None)\n",
    "            mse, nmse, r2, crps, f1 = None, None, None, None, None\n",
    "            if perf:\n",
    "                mse  = getattr(perf, \"mse\", None)\n",
    "                nmse = getattr(perf, \"nmse\", None)\n",
    "                r2   = getattr(perf, \"r2\", None)\n",
    "                crps = getattr(perf, \"crps\", None)\n",
    "                f1   = getattr(perf, \"f1\", None)\n",
    "                kl = getattr(perf, \"kl_divergence\", None)\n",
    "\n",
    "            # Create a performance threshold check\n",
    "            if kl is not None:\n",
    "                passed_metric_threshold = (kl <= kl_thr)\n",
    "            elif crps is not None:\n",
    "                passed_metric_threshold = (crps <= crps_thr)\n",
    "            elif f1 is not None:\n",
    "                passed_metric_threshold = (f1 >= f1_thr)\n",
    "            else:\n",
    "                passed_metric_threshold = None  # no available metric\n",
    "        \n",
    "        # Invertibility check\n",
    "        p_value = None\n",
    "        rejected = None\n",
    "        if invert_assumpt:\n",
    "            pnl = getattr(eval_result, \"pnl_assumptions\", {}).get(node, (None, False, None))\n",
    "            if isinstance(pnl, tuple):\n",
    "                p_value, rejected, _ = pnl\n",
    "\n",
    "        rows.append({\n",
    "            \"model\": model,\n",
    "            \"node\": node,\n",
    "            \"n_parents\": n_parents,\n",
    "            \"mechanism\": mech_type,\n",
    "            \"distribution/model\": pred_model,\n",
    "            \"model_params\": model_params,\n",
    "            \"mse\": mse if causal_mech else None,\n",
    "            \"nmse\": nmse if causal_mech else None, \n",
    "            \"r2\": r2 if causal_mech else None,\n",
    "            \"crps\": crps if causal_mech else None,\n",
    "            \"f1\": f1 if causal_mech else None,\n",
    "            \"kl\": kl if causal_mech else None,\n",
    "            \"p_value\": p_value if invert_assumpt else None,\n",
    "            \"invert_assumption_rejected\": rejected if invert_assumpt else None, \n",
    "            \"passed_metric_threshold\": passed_metric_threshold\n",
    "        })\n",
    "    \n",
    "    df_summary = pd.DataFrame(rows)\n",
    "\n",
    "    return df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Node Refinement ===\n",
    "def try_mechanism(mech_name, mech, scm, model, data, nodes, summary_fn, metric, kl_thr, crps_thr, f1_thr):\n",
    "    \"Helper function for mechanism evaluation\"\n",
    "\n",
    "    print(\"Refining nodes:\", nodes)\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        scm_local = copy.deepcopy(scm)\n",
    "        for node in nodes:\n",
    "            scm_local.set_causal_mechanism(node, mech)\n",
    "        gcm.fit(scm_local, data)\n",
    "        t1=time.time()\n",
    "        fit_time=t1-t0\n",
    "        print(f\"â±ï¸ fit time: {(fit_time):.2f}s\")\n",
    "        \n",
    "        # Evaluate nodes\n",
    "        node_eval = evaluate_nodes(scm_local, data, nodes, expand=True)\n",
    "        t2=time.time()\n",
    "        eval_time=t2-t1\n",
    "        print(f\"â±ï¸ eval time: {(eval_time):.2f}s\")\n",
    "\n",
    "        # Create node summary  \n",
    "        fit_summary = summary_fn(model, node_eval, scm_local, kl_thr, crps_thr, f1_thr, nodes, causal_mech=True, invert_assumpt=False)\n",
    "        metric_avg_nodes = fit_summary[metric].mean()\n",
    "        print(f\"{mech_name.upper()}: {metric} average = {metric_avg_nodes}\")\n",
    "        return mech_name, mech, metric, metric_avg_nodes, fit_summary\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ nodes failed with {mech_name} ({e})\")\n",
    "        return mech_name, mech, metric, None, None\n",
    "\n",
    "def refine_node_mechanisms(\n",
    "    scm, model, data, init_eval, kl_thr, crps_thr, f1_thr, nodes, mechanism_candidates, metric, summary_fn, stage_label, parallel=True):\n",
    "    \"\"\"\n",
    "    Generic refinement loop for a single node.\n",
    "    Integrates logging, reversion, and early stopping.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize\n",
    "    node_log = []\n",
    "    best_mech = None\n",
    "    init_metric_avg = init_eval[init_eval['node'].isin(nodes)][metric].mean()\n",
    "    best_metric_avg = init_metric_avg\n",
    "\n",
    "    # Parallel or serial evaluation\n",
    "    results = (\n",
    "        Parallel(n_jobs=min(6, max(1, len(mechanism_candidates))), backend=\"loky\", verbose=5)(\n",
    "            delayed(try_mechanism)(name, mech, scm, model, data, nodes, summary_fn, metric, kl_thr, crps_thr, f1_thr) for name, mech in mechanism_candidates.items()\n",
    "        )\n",
    "        if parallel else\n",
    "        [try_mechanism(name, mech, scm, model, data, nodes, summary_fn, metric, kl_thr, crps_thr, f1_thr) for name, mech in mechanism_candidates.items()]\n",
    "    )\n",
    "\n",
    "    # Find best mechanism by metric direction\n",
    "    for mech_name, mech, metric, new_metric_avg, fit_summary in results:\n",
    "        \n",
    "        current_mech_type, current_model_name, current_params = describe_mechanism(mech)\n",
    "        if metric in [\"kl\", \"crps\"]:\n",
    "            current_model_name = mech_name\n",
    "            if new_metric_avg < best_metric_avg:\n",
    "                improved = True\n",
    "            else: \n",
    "                improved = False\n",
    "        else:\n",
    "            if new_metric_avg > best_metric_avg:\n",
    "                improved = True\n",
    "            else:\n",
    "                improved = False\n",
    "\n",
    "        node_log.append({\n",
    "            \"model\": model,\n",
    "            \"nodes\": nodes,\n",
    "            \"metric\": metric,\n",
    "            \"initial_metric_avg\": init_metric_avg,\n",
    "            \"new_metric_avg\": new_metric_avg,\n",
    "            \"stage\": stage_label,\n",
    "            \"current_mechanism\": current_mech_type,\n",
    "            \"current_model\": current_model_name,\n",
    "            \"current_params\": json.dumps(current_params, default=str),\n",
    "            \"improved\": improved\n",
    "        })\n",
    "\n",
    "        if improved:\n",
    "            print(f\"âœ… {metric} nodes improved ({init_metric_avg} â†’ {new_metric_avg})\")\n",
    "            best_metric_avg, best_mech, best_summary = new_metric_avg, mech, fit_summary\n",
    "\n",
    "    # Final mechanism assignment  \n",
    "    if (metric in [\"kl\", \"crps\"] and (best_metric_avg >= init_metric_avg)) or (metric not in [\"kl\", \"crps\"] and (best_metric_avg <= init_metric_avg)):\n",
    "        print(f\"âš ï¸ {metric} nodes: performance worsened or did not improve â€” reverting.\")\n",
    "        \n",
    "        initial_mech_type, initial_model_name, initial_params = [], [], []\n",
    "        for node in nodes:\n",
    "            mech = scm.causal_mechanism(node)\n",
    "            mech_type, model_name, params = describe_mechanism(mech)\n",
    "            initial_mech_type.append(mech_type)\n",
    "            initial_model_name.append(model_name)\n",
    "            initial_params.append(params)\n",
    "        \n",
    "        node_log.append({\n",
    "            \"model\": model,\n",
    "            \"nodes\": nodes,\n",
    "            \"metric\": metric,\n",
    "            \"initial_metric_avg\": init_metric_avg,\n",
    "            \"new_metric_avg\": init_metric_avg,\n",
    "            \"stage\": stage_label,\n",
    "            \"current_mechanism\": initial_mech_type,\n",
    "            \"current_model\": initial_model_name,\n",
    "            \"current_params\": json.dumps(initial_params, default=str),\n",
    "            \"improved\": False\n",
    "        })\n",
    "    else:\n",
    "        # Need to assign best mech since the other assignment was on a copy of an SCM\n",
    "        print(f\"ðŸ”§ {metric} nodes: final mechanism set to {type(best_mech).__name__} \"\n",
    "            f\"({type(getattr(best_mech, 'prediction_model', getattr(best_mech, 'dist', None))).__name__})\")\n",
    "        for node in nodes:\n",
    "            scm.set_causal_mechanism(node, best_mech)\n",
    "\n",
    "\n",
    "    # Refit after assigning mechanisms\n",
    "    gcm.fit(scm, data)\n",
    "\n",
    "    return node_log\n",
    "\n",
    "def generate_continuous_mechs():\n",
    "    \"\"\"Generate continuous model candidates including hyperparameter sweeps.\"\"\"\n",
    "    configs = {}\n",
    "\n",
    "    # Linear baseline\n",
    "    configs[\"linear\"] = gcm.AdditiveNoiseModel(\n",
    "        prediction_model=gcm.ml.create_linear_regressor()\n",
    "    )\n",
    "\n",
    "    # Random Forests with varying n_estimators\n",
    "    for n in [50, 100, 200]:\n",
    "        key = f\"rf_{n}\"\n",
    "        configs[key] = gcm.AdditiveNoiseModel(\n",
    "            prediction_model=SklearnRegressionModel(\n",
    "                RandomForestRegressor(n_estimators=n, max_depth=8,  # Prevent overly deep trees\n",
    "                min_samples_leaf=5, n_jobs=-1, random_state=0)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Gradient Boosting\n",
    "    for lr in [0.01, 0.05, 0.1]:\n",
    "        key = f\"gbm_lr{lr}\"\n",
    "        configs[key] = gcm.AdditiveNoiseModel(\n",
    "            prediction_model=SklearnRegressionModel(\n",
    "                GradientBoostingRegressor(learning_rate=lr, n_estimators=150, max_depth=3, random_state=0)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # MLPs with varying hidden sizes\n",
    "    for size in [(20,), (50,), (100,)]:\n",
    "        key = f\"mlp_{size[0]}\"\n",
    "        configs[key] = gcm.AdditiveNoiseModel(\n",
    "            prediction_model=SklearnRegressionModel(\n",
    "                MLPRegressor(hidden_layer_sizes=size, max_iter=150, solver='adam', early_stopping=True, n_iter_no_change=5,\n",
    "                tol=1e-3, random_state=0)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return configs\n",
    "\n",
    "def generate_discrete_mechs():\n",
    "    \"\"\"Generate discrete model candidates including hyperparameter sweeps.\"\"\"\n",
    "    configs = {}\n",
    "\n",
    "    # Logistic Regression varying C\n",
    "    for c in [0.01, 0.1, 1.0, 10]:\n",
    "        key = f\"lg_{c}\"\n",
    "        configs[key] = gcm.ClassifierFCM(\n",
    "        classifier_model=SklearnClassificationModel(LogisticRegression(max_iter=500, C=c))\n",
    "        )\n",
    "\n",
    "    # Random Forest Classifier\n",
    "    for n in [50, 100, 200]:\n",
    "        key = f\"rf_{n}\"\n",
    "        configs[key] = gcm.ClassifierFCM(\n",
    "        classifier_model=SklearnClassificationModel(RandomForestClassifier(n_estimators=n, random_state=0)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return configs\n",
    "\n",
    "def evaluate_and_refine_scm(G, model, data, discrete_nodes, stage_label, kl_thr=1.0, crps_thr=0.35, f1_thr=0.6):\n",
    "    # Create SCM\n",
    "    scm = gcm.InvertibleStructuralCausalModel(G)\n",
    "    gcm.auto.assign_causal_mechanisms(scm, data, quality = AssignmentQuality.BETTER)\n",
    "    gcm.fit(scm, data)\n",
    "\n",
    "    # Run initial evaluation\n",
    "    eval_result = evaluate_nodes(scm, data, data.columns, expand=False)\n",
    "    eval_summary = summarize_and_flag_nodes(model, eval_result, scm, kl_thr, crps_thr, f1_thr)\n",
    "    eval_summary['stage'] = stage_label\n",
    "\n",
    "    # Determine which nodes need refinement, categorized by metric\n",
    "    nodes = eval_summary['node'].values\n",
    "    parents_count = np.array([len(list(G.predecessors(n))) for n in nodes])\n",
    "    is_discrete_mask = np.isin(nodes, discrete_nodes)\n",
    "\n",
    "    kl_mask = (parents_count == 0) & eval_summary['kl'].notna() & (eval_summary['kl'] > kl_thr)\n",
    "    crps_mask = (~is_discrete_mask) & eval_summary['crps'].notna() & (eval_summary['crps'] > crps_thr)\n",
    "    f1_mask = (is_discrete_mask) & eval_summary['f1'].notna() & (eval_summary['f1'] < f1_thr)\n",
    "\n",
    "    kl_nodes = nodes[kl_mask].tolist()\n",
    "    crps_nodes = nodes[crps_mask].tolist()\n",
    "    f1_nodes = nodes[f1_mask].tolist()\n",
    "\n",
    "    node_log = pd.DataFrame()\n",
    "    if not (len(kl_nodes) or len(crps_nodes) or len(f1_nodes)):\n",
    "        return scm, eval_summary, node_log\n",
    "\n",
    "    print(f\"Nodes flagged for KL refinement ({len(kl_nodes)}): {kl_nodes}\")\n",
    "    print(f\"Nodes flagged for RÂ² refinement ({len(crps_nodes)}): {crps_nodes}\")\n",
    "    print(f\"Nodes flagged for F1 refinement ({len(f1_nodes)}): {f1_nodes}\")\n",
    "\n",
    "    # Populate mechanism candidates\n",
    "    root_mechs = {\n",
    "        \"Bernoulli\": gcm.ScipyDistribution(dist=BernoulliDistribution()),\n",
    "        \"Beta\": gcm.ScipyDistribution(dist=BetaDistribution()),\n",
    "        \"GMM\": gcm.ScipyDistribution(dist=GaussianMixtureDistribution()),\n",
    "        \"Gaussian\": gcm.ScipyDistribution(dist=GaussianDistribution())\n",
    "    }\n",
    "    continuous_mechs = generate_continuous_mechs()\n",
    "    discrete_mechs = generate_discrete_mechs()\n",
    "\n",
    "    # Initialize logs in case no nodes trigger refinement\n",
    "    node_log_kl, node_log_crps, node_log_f1 = [], [], []\n",
    "\n",
    "    # Refine nodes in each list\n",
    "    print(f\"Refining nodes via KL\")\n",
    "    if len(kl_nodes) > 0:\n",
    "        node_log_kl = refine_node_mechanisms(\n",
    "            scm, model, data, eval_summary, kl_thr, crps_thr, f1_thr, kl_nodes, root_mechs, metric=\"kl\",\n",
    "            summary_fn=summarize_and_flag_nodes, stage_label=f\"{stage_label}_root_refine\", parallel=True\n",
    "        )\n",
    "    \n",
    "    print(f\"Refining nodes via CRPS\")\n",
    "    if len(crps_nodes) > 0:\n",
    "        node_log_crps = refine_node_mechanisms(\n",
    "            scm, model, data, eval_summary, kl_thr, crps_thr, f1_thr, crps_nodes, continuous_mechs, metric=\"crps\", \n",
    "            summary_fn=summarize_and_flag_nodes, stage_label=f\"{stage_label}_cont_refine\", parallel=True\n",
    "        )\n",
    "\n",
    "    print(f\"Refining nodes via F1\")\n",
    "    if len(f1_nodes) > 0:\n",
    "        node_log_f1 = refine_node_mechanisms(\n",
    "            scm, model, data, eval_summary, kl_thr, crps_thr, f1_thr, f1_nodes, discrete_mechs, metric=\"f1\",\n",
    "            summary_fn=summarize_and_flag_nodes, stage_label=f\"{stage_label}_disc_refine\", parallel=True\n",
    "        )\n",
    "    node_log = pd.concat([pd.DataFrame(node_log_kl), pd.DataFrame(node_log_crps), pd.DataFrame(node_log_f1)])\n",
    "    \n",
    "    return scm, eval_summary, node_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Benchmark Evaluation\n",
    "def run_benchmark_evaluation(scm, model, data, discrete_nodes, stage_level, forbidden_edges, post_fit_summary, n_bootstrap=20):\n",
    "    \"\"\"\n",
    "    Evaluate node-level predictive performance using a benchmark model\n",
    "    trained with all possible (non-forbidden) parents.\n",
    "    \n",
    "    Returns:\n",
    "        benchmark_results: list of dicts with per-node benchmark metrics\n",
    "        added_required: list of (p, node) edges to add\n",
    "        added_forbidden: list of (p, node) edges to forbid\n",
    "    \"\"\"\n",
    "    benchmark_results = []\n",
    "    added_required = []\n",
    "    added_forbidden = []\n",
    "\n",
    "    for node in scm.graph.nodes:\n",
    "        print(f\"benchmark for {node}\")\n",
    "        t0 = time.time()\n",
    "        parents = list(scm.graph.predecessors(node))\n",
    "\n",
    "        # Build simulated full-parent graph for this node\n",
    "        candidate_parents = [\n",
    "            p for p in scm.graph.nodes\n",
    "            if p != node and (p, node) not in forbidden_edges\n",
    "        ]\n",
    "        if not candidate_parents:\n",
    "            continue\n",
    "\n",
    "        X = data[candidate_parents]\n",
    "        y = data[node]\n",
    "        importance_accum = {p: 0.0 for p in X.columns}\n",
    "\n",
    "        # Select model + metric\n",
    "        if len(parents)==0:\n",
    "            bench_model = RandomForestRegressor(n_estimators=200, random_state=0)\n",
    "            metric_fn = lambda yt, yp: r2_score(yt, yp)\n",
    "            scm_metric = \"kl\"\n",
    "            metric = \"r2\"\n",
    "        elif node in discrete_nodes:\n",
    "            bench_model = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "            metric_fn = lambda yt, yp: f1_score(yt, yp, average=\"weighted\")\n",
    "            scm_metric = \"f1\"\n",
    "            metric = \"f1\"\n",
    "        else:\n",
    "            bench_model = RandomForestRegressor(n_estimators=200, random_state=0)\n",
    "            metric_fn = lambda yt, yp: r2_score(yt, yp)\n",
    "            scm_metric = \"crps\"\n",
    "            metric = \"r2\"\n",
    "\n",
    "        # Bootstrap importance accumulation\n",
    "        for _ in range(n_bootstrap):\n",
    "            X_bs, y_bs = resample(X, y, random_state=None)\n",
    "            bench_model.fit(X_bs, y_bs)\n",
    "            for p, imp in zip(X.columns, bench_model.feature_importances_):\n",
    "                importance_accum[p] += imp\n",
    "        print(f\"â±ï¸ fit time: {(time.time()-t0):.2f}s\")\n",
    "        t1 = time.time()\n",
    "\n",
    "        # Compute average importance\n",
    "        feat_importance_dict = {p: imp / n_bootstrap for p, imp in importance_accum.items()}\n",
    "        y_pred = bench_model.predict(X)\n",
    "        metric_val = metric_fn(y, y_pred)\n",
    "        print(f\"â±ï¸ evaluation time: {(time.time()-t1):.2f}s\")\n",
    "\n",
    "        # Compare to SCM performance\n",
    "        scm_score = post_fit_summary.loc[\n",
    "            post_fit_summary[\"node\"] == node, scm_metric\n",
    "        ].values[0] if node in post_fit_summary[\"node\"].values else None\n",
    "\n",
    "        # Get mechanism of SCM\n",
    "        mech = scm.causal_mechanism(node)\n",
    "        mech_type, model_name, model_params = describe_mechanism(mech)\n",
    "\n",
    "        # Option to manipulate graph manipulation based on feature importance with added_required output\n",
    "        top_parents = [p for p, imp in sorted(feat_importance_dict.items(), key=lambda x: -x[1]) if imp > 0.15]\n",
    "        top_parents_dict = {p: imp \n",
    "            for p, imp in sorted(feat_importance_dict.items(), key=lambda x: x[1], reverse=True) if imp > 0.15\n",
    "        }        \n",
    "        for p in top_parents:\n",
    "            if p not in scm.graph.predecessors(node):\n",
    "                added_required.append((p, node))\n",
    "\n",
    "        # Save results\n",
    "        benchmark_results.append({\n",
    "            \"model\": model,\n",
    "            \"node\": node,\n",
    "            \"stage\": stage_level,\n",
    "            \"n_scm_parents\": len(parents),\n",
    "            \"n_all_poss_parents\": len(candidate_parents),\n",
    "            \"scm_metric\": scm_metric,\n",
    "            \"scm_mechanism\": mech_type,\n",
    "            \"scm_model\": model_name,\n",
    "            \"scm_model_params\": json.dumps(model_params, default=str), \n",
    "            \"scm_score\": scm_score,\n",
    "            \"metric\": metric,\n",
    "            \"benchmark_score\": metric_val,\n",
    "            \"benchmark_top_parents\": top_parents_dict\n",
    "        })\n",
    "\n",
    "    return benchmark_results, added_required, added_forbidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "models = [\"pc\", \"fci\", \"lingam\", \"notears_linear\", \"notears_nonlinear\"]\n",
    "alpha = 0.1  # default 0.05, the higher the value the more edges\n",
    "bootstrap = 100 \n",
    "freq_thresholds = [0.2, 0.35, 0.5]\n",
    "KL_THRESHOLD = 1.0\n",
    "CRPS_THRESHOLD = 0.35\n",
    "F1_THRESHOLD = 0.6\n",
    "\n",
    "gcm.util.general.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸ started at: 2025-11-28 11:44:49\n",
      "â±ï¸ model loop started at: 2025-11-28 11:44:49\n",
      "\n",
      "=== Running PC | Î±=0.1 | boot=100 ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting single causal discovery run\n",
      "Worker memory (MB): 392.085504\n",
      "# Forbidden edges: 717\n",
      "# Required edges: 3\n",
      "Starting single causal discovery run\n",
      "Worker memory (MB): 391.90528\n",
      "# Forbidden edges: 717\n",
      "# Required edges: 3\n",
      "Starting single causal discovery run\n",
      "Worker memory (MB): 393.13408\n",
      "# Forbidden edges: 717\n",
      "# Required edges: 3\n",
      "Starting single causal discovery run\n",
      "Worker memory (MB): 393.101312\n",
      "# Forbidden edges: 717\n",
      "# Required edges: 3\n",
      "Starting single causal discovery run\n",
      "Worker memory (MB): 390.283264\n",
      "# Forbidden edges: 717\n",
      "# Required edges: 3\n",
      "Starting single causal discovery run\n",
      "Worker memory (MB): 392.101888\n",
      "# Forbidden edges: 717\n",
      "# Required edges: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Depth=1, working on node 10:  26%|â–ˆâ–ˆâ–Œ       | 11/43 [00:17<01:31,  2.87s/it] "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"â±ï¸ started at:\", datetime.datetime.fromtimestamp(start).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# Initialize variables\n",
    "discrete_nodes = ['is_female', 'long_covid', 'me_cfs', 'fibromyalgia', 'dysautonomia', 'period_at_covid_start', 'pre_crash', \n",
    "                 'acute_crash', 'post_crash', 'pre_noncovid_infection', 'acute_noncovid_infection', 'post_noncovid_infection']\n",
    "consensus_graphs = {}\n",
    "fit_summary, benchmark_summary, node_log_summary, edge_summary = [], [], [], []\n",
    "model_datasets = {}\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    model_start = time.time()\n",
    "    print(\"â±ï¸ model loop started at:\", datetime.datetime.fromtimestamp(model_start).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    print(f\"\\n=== Running {model.upper()} | Î±={alpha} | boot={bootstrap} ===\")\n",
    "\n",
    "    # Initialize variables\n",
    "    fit_summary_model = []\n",
    "    benchmark_summary_model = []\n",
    "    node_log_summary_model = []\n",
    "\n",
    "    # --- Run causal discovery sequentially ---\n",
    "    features = final_df_std.columns.tolist()\n",
    "    edge_results, stability = run_bootstrap_discovery(model, final_df_std, alpha, bootstrap, stage_level=\"initial\", required=None, forbidden=None)\n",
    "    edge_results_df = pd.DataFrame(edge_results)\n",
    "    causal_disc_time = time.time()\n",
    "    print(f\"â±ï¸ causal discovery time: {(causal_disc_time-model_start):.2f}s\")\n",
    "\n",
    "    # Save results\n",
    "    initial_edge_summary, initial_conf_summary = summarize_edge_stability(edge_results_df)\n",
    "    initial_edge_summary['stage'] = 'initial'\n",
    "    initial_conf_summary['stage'] = 'initial'\n",
    "    edge_summary.append(initial_edge_summary)\n",
    "\n",
    "    # --- Iterate through results to analyze and manipulate models/graphs ---\n",
    "    for freq_threshold in freq_thresholds:\n",
    "        \n",
    "        threshold_time = time.time()\n",
    "        print(\"â±ï¸ threshold loop started at:\", datetime.datetime.fromtimestamp(threshold_time).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "        # --- Create graphical representations of discovery results ---\n",
    "        # Calculate stability \n",
    "        subset = edge_results_df[(edge_results_df[\"model\"] == model) & (edge_results_df[\"alpha\"] == alpha) & (edge_results_df[\"n_bootstrap\"] == bootstrap)]\n",
    "        stability_final = subset.iloc[0][\"stability\"]\n",
    "\n",
    "        # Populate background knowledge\n",
    "        forbidden, required = build_background_knowledge(features)\n",
    "        forbidden_edges = set(forbidden)  # convert to set for fast lookup\n",
    "        required_edges = set(required)\n",
    "\n",
    "        # Build graph with stability scores\n",
    "        G = build_consensus_graph(model, stability_final, required_edges, freq_threshold=freq_threshold) \n",
    "\n",
    "        # Add isolated nodes before plotting\n",
    "        all_nodes = set(final_df_std.columns) \n",
    "        graph_nodes = set(G.nodes)\n",
    "        missing_nodes = all_nodes - graph_nodes\n",
    "        if missing_nodes:\n",
    "            print(f\"Adding {len(missing_nodes)} isolated nodes to graph: {missing_nodes}\")\n",
    "            G.add_nodes_from(missing_nodes) \n",
    "\n",
    "        # Plot and store graph    \n",
    "        key = (model, alpha, bootstrap, freq_threshold)\n",
    "        consensus_graphs[key] = G\n",
    "        graph_dir = Path(\"graph_objects\")\n",
    "        graph_dir.mkdir(exist_ok=True)\n",
    "        key_str = f\"{model}_b{bootstrap}_{freq_threshold}\".replace(\".\", \"_\")\n",
    "        with open(graph_dir / f\"{key_str}_initial.pkl\", \"wb\") as f:\n",
    "            pickle.dump(G, f)\n",
    "        plot_consensus_graph(G, model=model, threshold=freq_threshold, outdir=\"graph_plots\", stage_level=\"initial\", alpha=alpha, n_bootstrap=bootstrap)\n",
    "\n",
    "        graph_time = time.time()\n",
    "        print(f\"â±ï¸ graph building time: {(graph_time-threshold_time):.2f}s\")\n",
    "\n",
    "        # --- Graph evaluation and refinement ---\n",
    "        print(f\"\\n===== Evaluating Graph ({key[0]}): alpha={key[1]}, bootstraps={key[2]}, threshold={key[3]} =====\")\n",
    "        scm_post_nr, init_fit_summary, node_log = evaluate_and_refine_scm(G, model, final_df_std, discrete_nodes, stage_label='initial', \n",
    "        kl_thr=KL_THRESHOLD, crps_thr=CRPS_THRESHOLD, f1_thr=F1_THRESHOLD)\n",
    "\n",
    "        weak_nodes = init_fit_summary.loc[\n",
    "            (init_fit_summary['crps']<CRPS_THRESHOLD)|((init_fit_summary[\"kl\"] > KL_THRESHOLD))|( (init_fit_summary[\"f1\"] < F1_THRESHOLD))][\"node\"].tolist()\n",
    "\n",
    "        print(f\"âš ï¸ Initial weak nodes {len(weak_nodes)}: {weak_nodes}\")\n",
    "        \n",
    "        # Post node refinement evaluation\n",
    "        print(\"\\nðŸ“Š Running post node refinement evaluation...\")\n",
    "        post_nr_eval = evaluate_nodes(scm_post_nr, final_df_std, final_df_std.columns, expand=False)\n",
    "        post_nr_fit_summary = summarize_and_flag_nodes(model, post_nr_eval, scm_post_nr, kl_thr=KL_THRESHOLD, crps_thr=CRPS_THRESHOLD, f1_thr=F1_THRESHOLD)\n",
    "        post_nr_fit_summary['stage'] = 'post node refinement'\n",
    "\n",
    "        node_eval_time = time.time()\n",
    "        print(f\"â±ï¸ node evaluation time: {(node_eval_time-graph_time):.2f}s\") \n",
    "\n",
    "        # Save the refined graph structure and SCM (structure + mechanisms) separately\n",
    "        G_refined = scm_post_nr.graph\n",
    "        with open(graph_dir / f\"{key_str}_refined.pkl\", \"wb\") as f:\n",
    "            pickle.dump(G_refined, f)\n",
    "        print(f\"âœ“ Saved refined graph structure to {key_str}_refined.pkl\")\n",
    "        with open(graph_dir / f\"{key_str}_refined_scm.pkl\", \"wb\") as f:\n",
    "            pickle.dump(scm_post_nr, f)\n",
    "        print(f\"âœ“ Saved refined SCM to {key_str}_refined_scm.pkl\")\n",
    "\n",
    "        weak_nodes = post_nr_fit_summary.loc[\n",
    "            (post_nr_fit_summary['crps']<CRPS_THRESHOLD)|((post_nr_fit_summary[\"kl\"] > KL_THRESHOLD))|( (post_nr_fit_summary[\"f1\"] < F1_THRESHOLD))][\"node\"].tolist()\n",
    "        print(f\"âš ï¸ Remaining weak nodes {len(weak_nodes)}: {weak_nodes}\")\n",
    "\n",
    "        # --- Benchmark evaluation for nodes ---\n",
    "        benchmark_results, added_required, added_forbidden = run_benchmark_evaluation(scm_post_nr, model, final_df_std, discrete_nodes, stage_level='initial', \n",
    "        forbidden_edges=forbidden_edges, post_fit_summary=post_nr_fit_summary, n_bootstrap=20)\n",
    "\n",
    "        bench_eval_time = time.time()\n",
    "        print(f\"â±ï¸ benchmark evaluation time: {(bench_eval_time-node_eval_time):.2f}s\")\n",
    "\n",
    "        # --- Save results ---\n",
    "        thr_fit_summary = pd.concat([init_fit_summary, post_nr_fit_summary])\n",
    "        thr_fit_summary['threshold'] = freq_threshold\n",
    "        fit_summary_model.append(thr_fit_summary)\n",
    "        fit_summary.append(thr_fit_summary)\n",
    "\n",
    "        thr_benchmark_summary = pd.DataFrame(benchmark_results)\n",
    "        thr_benchmark_summary['threshold'] = freq_threshold\n",
    "        benchmark_summary_model.append(thr_benchmark_summary)\n",
    "        benchmark_summary.append(thr_benchmark_summary)\n",
    "\n",
    "        thr_node_log_summary = pd.DataFrame(node_log)\n",
    "        thr_node_log_summary['threshold'] = freq_threshold\n",
    "        node_log_summary_model.append(thr_node_log_summary)\n",
    "        node_log_summary.append(thr_node_log_summary)\n",
    "\n",
    "    # Export\n",
    "    os.makedirs(\"summaries\", exist_ok=True)    \n",
    "    initial_edge_summary.to_csv(f\"summaries/causal_disc_edge_summary_{model}_b{bootstrap}.csv\", index=False)\n",
    "    if model==\"fci\":\n",
    "        initial_conf_summary.to_csv(f\"summaries/causal_disc_conf_summary_{model}_b{bootstrap}.csv\", index=False)\n",
    "    pd.concat(fit_summary_model).to_csv(f\"summaries/scm_fit_summary_{model}_b{bootstrap}.csv\", index=False)\n",
    "    pd.concat(benchmark_summary_model).to_csv(f\"summaries/benchmark_comparison_{model}_b{bootstrap}.csv\", index=False)\n",
    "    pd.concat(node_log_summary_model).to_csv(f\"summaries/scm_node_assignment_log_{model}_b{bootstrap}.csv\", index=False)\n",
    "\n",
    "pd.concat(edge_summary).to_csv(f\"summaries/causal_disc_edge_summary_b{bootstrap}.csv\", index=False)   \n",
    "pd.concat(fit_summary).to_csv(f\"summaries/scm_fit_summary_b{bootstrap}.csv\", index=False)\n",
    "pd.concat(benchmark_summary).to_csv(f\"summaries/benchmark_comparison_b{bootstrap}.csv\", index=False)\n",
    "pd.concat(node_log_summary).to_csv(f\"summaries/scm_node_assignment_log_b{bootstrap}.csv\", index=False)\n",
    "print(f\"âœ… Results saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynewenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
